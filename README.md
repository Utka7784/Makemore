<h1 align="center">ğŸ§  Makemore â€” Character-Level Language Model (From Scratch)</h1>

<p align="center">
  <img src="https://img.shields.io/badge/Made%20with-Python-blue?logo=python" />
  <img src="https://img.shields.io/badge/Built%20with-Jupyter%20Notebook-orange?logo=jupyter" />
  <img src="https://img.shields.io/badge/Framework-PyTorch-red?logo=pytorch" />
  <img src="https://img.shields.io/badge/Project-Type-From%20Scratch-success" />
  <img src="https://img.shields.io/badge/License-MIT-green" />
</p>

<p align="center">
  <img src="outputs/sample_generation_preview.png" alt="Sample Output Preview" width="600"/>
</p>

---

## ğŸ“˜ Overview

Implementation of a **character-level language model** entirely from scratch in pure **Python + PyTorch**, following  
[Andrej Karpathyâ€™s â€œMakemoreâ€](https://www.youtube.com/watch?v=PaCmpygFfXo) from the *Neural Networks: Zero to Hero* series.

This project trains a small GPT-like model to learn and generate realistic-looking names one character at a time â€” the fundamental concept behind how **Large Language Models (LLMs)** predict text.

---

## ğŸ§© Project Structure
makemore/
â”œâ”€â”€ makemore.ipynb # Main notebook: end-to-end implementation
â”œâ”€â”€ names.txt # Dataset (list of training names)
â”œâ”€â”€ outputs/
â”‚ â”œâ”€â”€ loss_curve.png
â”‚ â””â”€â”€ sample_generation_preview.png
â””â”€â”€ README.md

yaml
Copy code

---

## ğŸš€ How to Run
```bash
# 1. Clone repository
git clone https://github.com/Utka7784/Neural-Network.git
cd Neural-Network

# 2. Install dependencies
pip install torch matplotlib numpy jupyter

# 3. Launch notebook
jupyter notebook makemore.ipynb
Execute cells sequentially â€” every block builds up from tokenization to full text generation.

ğŸ§  Key Concepts Implemented
Character-level tokenization

Feed-forward MLP architecture

Embeddings and activation functions

Cross-entropy loss and optimization

Sampling & temperature-based text generation

ğŸ“Š Example Output
Prompt	Generated Names
ann	annika, annel, annabel
mic	michel, micka, michal

(Trained on names.txt dataset)

ğŸ’¡ Learnings
How neural nets perform next-character prediction

Relationship between N-gram models and Transformers

Fundamentals behind GPT-style next-token generation

Foundation for understanding and securing LLM architectures

ğŸ§­ Next Steps
 Expand to trigram context

 Add positional encodings

 Transition to attention mechanism

 Explore prompt-level adversarial robustness

ğŸ‘¨â€ğŸ’» Author
Utkarsh Walchale
Cybersecurity & AI Engineer | LLM Security Researcher

Exploring how neural networks learn, fail, and can be secured â€” one character at a time.

<p align="center"> <a href="https://linkedin.com/in/utkarshwalchale"><img src="https://img.shields.io/badge/LinkedIn-Utkarsh Walchale-blue?logo=linkedin"></a> <a href="https://github.com/Utka7784"><img src="https://img.shields.io/badge/GitHub-Utka7784-black?logo=github"></a> </p>
âš–ï¸ License
Released under the MIT License â€” free to use, modify, and learn from.

yaml
Copy code

---

## âœ… Steps to Add
1. Create or open `README.md` in your Makemore folder.  
2. Paste the above markdown exactly.  
3. (Optional) add a sample image named `sample_generation_preview.png` inside `outputs/` â€” it will display automatically.  
4. Push to GitHub:
```bash
git add README.md
git commit -m "Add Makemore project README with badges and preview"
git push -u origin main
