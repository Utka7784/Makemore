# ðŸ§  Makemore â€” Character-Level Language Model (From Scratch)

> Implementation of a **character-level language model** from scratch in pure Python and PyTorch, following [Andrej Karpathyâ€™s "Makemore"](https://www.youtube.com/watch?v=PaCmpygFfXo) from the **Neural Networks: Zero to Hero** series.

---

## Overview

This project builds a **mini GPT-style model** capable of learning to generate names (or text) one character at a time.  
It demonstrates how a neural network can be trained to predict the next character in a sequence â€” forming the foundation of how **Large Language Models (LLMs)** work internally.

I implemented and experimented with this entirely in **Jupyter Notebook** to understand every component â€” data processing, embeddings, feed-forward layers, and loss optimization â€” all built step-by-step.

---

## ðŸ§© Project Structure

makemore/
â”œâ”€â”€ makemore.ipynb # Main Jupyter Notebook (complete implementation)
â”œâ”€â”€ names.txt # Dataset (list of training words)
â”œâ”€â”€ outputs/ # Generated samples and loss plots
â””â”€â”€ README.md

yaml
Copy code

---

## ðŸš€ How to Run

1. **Clone the repository**
   ```bash
   git clone https://github.com/<your-username>/makemore-from-scratch.git
   cd makemore-from-scratch
Install dependencies

bash
Copy code
pip install torch matplotlib numpy jupyter
Run the notebook

bash
Copy code
jupyter notebook makemore.ipynb
Follow the cells step-by-step â€” each section builds upon the previous, starting from data encoding to a working generative model.

ðŸ§  What This Project Teaches
Building tokenizers and character-to-index mappings

Implementing forward and backward passes for a multi-layer perceptron

Understanding embeddings, non-linear activations, and softmax

Training using negative log-likelihood loss

Generating new samples with sampling and temperature scaling

Connecting character-level models to transformer-based LLMs

ðŸ“Š Sample Output
Input Prompt	Generated Text
ann	annah, annie, annika
mic	michael, micka, michel

(Generated by the model after training on names.txt)

ðŸ’¡ Learnings & Insights
How LLMs learn sequence dependencies through next-token prediction

How data encoding and embeddings affect model performance

Why attention-based models like GPT generalize this concept efficiently

Foundation for understanding Transformer internals for LLM Security research

ðŸ§© Next Steps
 Extend to bigram and trigram context windows

 Implement positional encoding

 Build a mini-Transformer using PyTorch

 Explore adversarial examples and data poisoning in text generation

ðŸ‘¨â€ðŸ’» Author
Utkarsh Walchale
Cybersecurity & AI Engineer | LLM Security Enthusiast

Exploring how neural networks learn, fail, and can be secured â€” one character at a time.

ðŸ“¬ LinkedIn â€¢ GitHub

âš–ï¸ License
This project is licensed under the MIT License.

yaml
Copy code

---

## âœ… To Add It to GitHub
From your Jupyter Notebook folder:
```bash
echo "" > README.md  # create file if not present
